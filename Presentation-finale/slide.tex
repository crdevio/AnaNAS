\documentclass{beamer}
\usetheme{metropolis}
\usepackage{subcaption}

% Configuration des couleurs modernes
\definecolor{myblue}{RGB}{173, 216, 230} % Bleu pastel
\definecolor{mymagenta}{RGB}{238, 130, 238} % Magenta doux
\definecolor{mywhite}{RGB}{255, 255, 255} % Blanc pur
\definecolor{mygray}{RGB}{245, 245, 245} % Gris clair
\definecolor{darkblue}{RGB}{25, 25, 112} % Bleu foncé
\definecolor{darkviolet}{RGB}{148, 0, 211} % Violet foncé
\definecolor{goldenrod}{RGB}{218, 165, 32} % Or doux

% Arrière-plan sobre et clair
\usepackage{tikz}

\addtobeamertemplate{background canvas}{}{
	\begin{tikzpicture}[remember picture, overlay]
		\fill[mywhite] (current page.south west) rectangle (current page.north east);
		\begin{scope}[blend mode=overlay]
			\shade[inner color=myblue!60, outer color=mywhite] (current page.south west) rectangle (current page.north east);
			\shade[inner color=mymagenta!60, outer color=mywhite] (current page.north east) rectangle (current page.south west);
		\end{scope}
	\end{tikzpicture}
}

% Personnalisation des couleurs dans metropolis
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{frametitle}{bg=mywhite, fg=black}
\setbeamercolor{title separator}{fg=mymagenta}
\setbeamercolor{progress bar}{fg=myblue, bg=mymagenta}
\setbeamercolor{block title}{bg=myblue, fg=black}
\setbeamercolor{block body}{bg=mywhite, fg=black}
\setbeamercolor{alerted text}{fg=darkviolet!70}

% Personnalisation des typos
\setbeamerfont{frametitle}{size=\Large,series=\bfseries}
\setbeamerfont{title}{size=\Huge,series=\bfseries}

% Apparence des transitions entre les frames
\metroset{progressbar=frametitle,numbering=fraction}

% Commande pour ajuster le séparateur du titre
\metroset{titleformat frame=smallcaps}

% Début du document
\begin{document}
	
	\title{Ananas : Apprentissage de conduite automatique par Deep Q Learning}
	\subtitle{Deep Learning - Projet}
	\author{Clément, Grégoire, Nathan}
	\date{January 31, 2025}
	
	\maketitle
	
	\section{Introduction}
	
	\begin{frame}{But du projet}
		
		Implémenter une voiture qui apprend à conduire en utilisant le Deep Reinforcement Learning.
		
		Pour des raisons d'efficacité temporelle, reimplémentation de notre propre environement plutôt que d'en utiliser un déjà tout fait
		
		Source principale : \textit{Playing Atari with Deep Reinforcement Learning}
		
	\end{frame}
	
	\section{Reinforcement learning}
	
	\begin{frame}{A quoi ça sert}
		
		Reinforcement learning : Agent qui évolue dans un environement \\
		Pour chaque action, récompenses ou pénalité. \\
		Objectif : Maximiser les récompense
		
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{img/RL.png}
		\end{figure}
	
	\end{frame}
	
	\begin{frame}{Formulation mathématiques}
		
		L'agent intéragit avec un environement stochastique $\mathcal{E}$ dans lequel il joue des partie composé d'un ensemble d'état, de score et d'action
		
		A chaque étape, il sélectionne une action $a_t$ parmis un ensemble $A = \{1, ..., K\}$ d'action légale
		
		A chaque étape, l'agent a accès à un ensemble d'information $x_t \in \mathbb{R}^d$.
	
	\end{frame}
	
	\begin{frame}{Objectif de l'agent}
		
		On définit le rendement futur attendu par
		$$R_t = \sum_{t' = t}^{T}\gamma^{t' - t}r_{t'}$$
		où $T$ est l'instant auquel le jeu s'arrête. \\
		But de l'agent : maximiser ce rendement. 
				
	\end{frame}
	
	\begin{frame}{Optimal action value function}
		
		On définit l'optimal action value function $Q*(s', a')$ qui donne le bon choix à faire en fonction de la situation actuelle, cest à dire
		$$Q^*(s, a)=  \mathbb{E}_{\pi}[R_t | s_t = s, a_t = a, \pi]$$\\
		Si on dispose de cette fonction, alors il suffit de faire le meilleur choix à chaque fois.
	\end{frame}
	
	\begin{frame}{Comment calculer cette fonction}
		
		Cette fonction suit l'équation de Bellman:	
		
		$$Q^*(s, a) = \mathbb{E}_{s' \sim \mathcal{E}}[r + \gamma \cdot \max_{a'}Q^*(s', a') | s, a]$$
		Alors si on pose
		$$Q_{i + 1}(s, a) = \mathbb{E}[r + \gamma \max_{a'}Q_i(s', a') | s, a]$$
		$Q_i$ tend vers $Q*$ lorsque $i$ tend vers $+\infty$
		Problème : Trop dur à estimer, trop de situations différentes possibles
	\end{frame}
	
	\begin{frame}{Estimation de la fonciton par un réseau}
		
		Solution : On approxime $Q^*$ par un réseau de neurone $Q(s, a, \theta)$
		On l'entraîne en minimisant les fonctions
		$$L_i(\theta_i) = \mathbb{E}_{s, a \sim \rho(.)}[(y_i - Q(s, a; \theta_i))^2]$$
		
	\end{frame}
	
	\section{Deep Q Learning}
	
	\begin{frame}{Equations blabla comment ça marche}
		
	\end{frame}
	
	\section{Environement de simulation}
	
	\begin{frame}{Pourquoi recoder notre propre enviornement}
		
		Inconvénient d'un émulateur : Temps d'accès potentiellement long, peu de flexibilité sur les informations qu'on peut passer à notre modèle donc moins d'experimentations possibles
		
		En recodant notre simulation, on peut contourner tous ces problèmes 
		
	\end{frame}
	
	\begin{frame}{Notre simulation}
		
		La on met des images de différents exemples
		
	\end{frame}
	
	\begin{frame}{Entraînement, abblation study et tout}
	\end{frame}
	
	\begin{frame}{Conclusion}
		
		
		
	\end{frame}
	
\end{document}